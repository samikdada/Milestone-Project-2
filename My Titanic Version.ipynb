{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.ensemble import (AdaBoostClassifier,RandomForestClassifier,ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = [train_df, test_df]\n",
    "Passenger_Id_train = train_df['PassengerId']\n",
    "Passenger_Id_test = test_df['PassengerId']\n",
    "train_df.drop(['PassengerId'], axis=1, inplace=True)\n",
    "test_df.drop(['PassengerId'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Embarked'].fillna('S', inplace= True)\n",
    "    dataset['Has_Cabin'] = dataset['Cabin'].apply(lambda x : 0 if type(x) == float else 1)\n",
    "    dataset['Family_Size'] = dataset['Parch'] + dataset['SibSp'] + 1\n",
    "    dataset['Is_Alone'] = dataset['Family_Size'].apply(lambda x: 1 if x == 1 else 0)    \n",
    "    dataset.drop(['Cabin', 'Ticket'], inplace=True, axis=1)\n",
    "    \n",
    "#Dividing the Age and the Fare column into Numerical Categories\n",
    "    dataset['Age'].fillna(dataset['Age'].mean(),inplace = True)\n",
    "    dataset['Categorical_Age'] = pd.cut(dataset['Age'],5)\n",
    "    \n",
    "    dataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].median())\n",
    "    dataset['Categorical_Fare'] = pd.cut(dataset['Fare'], 4)\n",
    "    \n",
    "    dataset.loc[(dataset['Fare']< 128.082), 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] >=128.082) & (dataset['Fare']< 256.165), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] >=256.165) & (dataset['Fare']< 384.247), 'Fare'] = 2\n",
    "    dataset.loc[(dataset['Fare'] >=384.247) & (dataset['Fare']< 513), 'Fare'] = 3   \n",
    "    dataset.loc[(dataset['Age']< 16.336), 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] >=16.336) & (dataset['Age']< 32.252), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] >=32.252) & (dataset['Age']< 48.168), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] >=48.168) & (dataset['Age']< 64.084), 'Age'] = 3\n",
    "    dataset.loc[(dataset['Age'] >=64.084) & (dataset['Age']< 81), 'Age'] = 4\n",
    "    dataset.drop(['Categorical_Age', \"Categorical_Fare\"], axis= 1, inplace= True)\n",
    "    \n",
    "#Extracting the different titles from Name column and assigning categories to them\n",
    "    dataset['Title'] = [i.split(',')[1] for i in [i[0] for i in dataset['Name'].apply(str.split, args=('.'))]]\n",
    "for title, index in zip(dataset['Title'].values, np.arange(len(dataset['Title']))):\n",
    "    for (key, value) in saldict2.items():\n",
    "        if title in value:\n",
    "            dataset['Title'][index] = key\n",
    "            break\n",
    "\n",
    "dataset['Title'] = dataset['Title'].apply(lambda x: 4 if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "saldict2 = {1: [' Mr', ' Master', ' Don', ' Sir'], 2: [' Mrs', ' Miss', ' Ms', ' Lady', ' the Countess'], \n",
    "            3: [' Dr', ' Major', ' Col', ' Capt',]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(train_df,columns=['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Title', 'Has_Cabin', 'Is_Alone'], drop_first=True)\n",
    "test_df = pd.get_dummies(test_df,columns=['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Title', 'Has_Cabin', 'Is_Alone'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnHelper():\n",
    "    \n",
    "    def __init__(self, clf, params=None):\n",
    "        self.clf = clf(**params)\n",
    "        \n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "n_split = 5\n",
    "kfold = KFold(n_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oof_Cal(clf, x_train, y_train, test_df):\n",
    "    n_split = 5\n",
    "    oof_train = np.zeros(x_train.shape[0],)\n",
    "    oof_test = np.zeros(test_df.shape[0],)\n",
    "    oof_test_svf = np.empty((n_split, test_df.shape[0]))\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(x_train)):\n",
    "        \n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "    \n",
    "        clf.fit(x_tr, y_tr)\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_svf[i, :] =  clf.predict(test_df)\n",
    "        oof_test = oof_test_svf.mean(axis=0)\n",
    "        \n",
    "    return oof_train.reshape(-1,1), oof_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "    }\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.drop('Survived', axis= True).values\n",
    "y_train = train_df['Survived'].values\n",
    "x_test = test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb = SklearnHelper(AdaBoostClassifier, ada_params)\n",
    "rf = SklearnHelper(RandomForestClassifier, rf_params)\n",
    "et = SklearnHelper(ExtraTreesClassifier, et_params)\n",
    "gb = SklearnHelper(GradientBoostingClassifier, gb_params)\n",
    "sv = SklearnHelper(SVC, svc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e5565890\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    }
   ],
   "source": [
    "adb_oof_train, adb_oof_test = oof_Cal(adb, x_train, y_train, x_test)\n",
    "rf_oof_train, rf_oof_test = oof_Cal(rf, x_train, y_train, x_test)\n",
    "et_oof_train, et_oof_test = oof_Cal(et, x_train, y_train, x_test)\n",
    "gb_oof_train, gb_oof_test = oof_Cal(gb, x_train, y_train, x_test)\n",
    "sv_oof_train, sv_oof_test = oof_Cal(sv, x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate([adb_oof_train, rf_oof_train, et_oof_train, gb_oof_train, sv_oof_train], axis=1)\n",
    "x_test = np.concatenate([adb_oof_test, rf_oof_test, et_oof_test, gb_oof_test, sv_oof_test], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
